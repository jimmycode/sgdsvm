
SGD-2.0
-------

L. Bottou, October 2011



1. INTRODUCTION

The goal of this package is to illustrate the efficiency of stochastic
gradient descent for large-scale learning tasks.  

Two algorithms,

    * Stochastic gradient descent (SGD), and
    * Averaged stochastic gradient descent (ASGD),

are applied to two well known problems

    * Linear Support Vector Machines, and
    * Conditional Random Fields.

The corresponding programs are designed for simplicity and readability.  In
particular they avoid optimizations that would made the programs less
readable.  The sole exception is the handling of sparse training data.



2. DATASETS

The programs are demonstrated using a number of standard datasets.

* The RCV1-V2 dataset.
* The ALPHA and WEBSPAM datasets from the first Pascal Large Scale Learning Challenge.
* The dataset associated with the CONLL2000 chunking task.

These datasets are available from the web.  File "data/README" contains
instructions for downloading.  The Pascal datasets must be 
preprocessed using a relatively slow python script.



3. ALGORITHMS

Unlike most optimization algorithm, each iteration of these stochastic
algorithms process a single example and update the parameters.  Although the
theory calls for picking a random example at each iteration, this
implementation performs sequential passes over randomly shuffled training
examples. This process is in fact more effective in practice.  Each pass is
called an epoch.

Assume we have an objective function of the form
 
     Obj(w) = 1/2 lambda w^2  + 1/n sum_i=1^n L(z_i,w)

where w is the parameter, {z_1,...,z_n} are the training examples, 1/2 \lambda
w^2 a regularization term, and L(z,w) is the loss function.  

Each iteration of the SGD algorithm picks a single example z and updates the
parameter using the formula

          SGD:     w := w - eta_t [ w + dL/dw(z,w) ]

The trick of course is to choose the gain sequence eta_t wisely.  We use the
formula eta_t = eta_0 / (1 + lambda eta_0 t), and we pick eta_0 by trying
several gain values on a subset of the training data.

The ASGD algorithm maintains two parameter vectors. 
The first parameter vector, w, is updated like the SGD parameter.  
The second parameter vector, a, computes an average of the previous values of w.

          ASGD:     w := w - eta_t [ w + dL/dw(z,w) ]
                    a := a + mu_t [ w - a ]

The sequence mu_t = 1/t ensures that a is the average of all the previous
values of w.  This algorithm has been shown to work extremely well (Polyak and
Juditsky, 1992) provided that the sequence eta_t decreases with exactly the
right speed.  We follow (Xu, 2010) and choose eta_t = eta_0 / (1 + lambda eta0
t) ^ 0.75 and we pick eta_0 by trying several gain values on a subset of the
training data.  In addition, before starting the averaging process, we iterate
on a substantial fraction of the data using mu=1.



4. SUPPORT VECTOR MACHINES

The directory "svm" contains programs to train a L2-regularized linear model
for binary classification tasks. Compilation time switches determine whether
the models include a bias term, whether the bias term is regularized, and
which loss function should be used.  The default is to use an unregularized
bias term using the log-loss function L(x,y,w) = log(1+exp(-ywx)).

Compiling under Unix is achieved using the traditional command "make".
Compilation switches can be conveniently given as follows:

$ make clean; make OPT='-DLOSS=HingeLoss -DBIAS=0'

Microsoft Visual Studio project files are available in the "win" directory.
You then need to copy the executables into the "svm" directory.

The preprocessing programs "prep_rcv1", "prep_alpha", and "prep_webspam" read
the original data files and produce binary files readable by the "svmsgd" and
"svmasgd" programs. See file "svm/README" for details about these programs and
their usage for each of the datasets. Here is a brief timing comparison of
various algorithms for training linear svms on the RCV1-V2 task.

- SGD, ASGD:               ~2 seconds.
- svmlight:             23642 seconds.
- svmperf:                 66 seconds.
- libsvm(tron):            30 seconds.

Note that recent versions of libsvm implement a dual coordinate ascent
algorithm that is related to stochastic gradient descent.



5. CONDITIONAL RANDOM FIELDS

The directory "crf" contains programs for learning the CONLL2000 chunking task
<http://www.cnts.ua.ac.be/conll2000/chunking>.  This program takes data files
and template files and produces tagging files similar to those of Taku Kudo's
CRF++ program described at <http://crfpp.sourceforge.net/>.  However it takes
gzipped data files instead of plain files.

Compiling under Unix is achieved using the traditional command "make".
Microsoft Visual Studio project files are available in the "win" directory.
You then need to copy the "crfsgd" and "crfasgd" executables into the "crf"
directory and you need to install Perl if you want to run the standard
evaluation script "conlleval"

File "crf/README" provides detailed information about these programs and their
usage. Here is a brief timing comparison of various algorithms for training a
CRF on this benchmark task.


crf++ (lbfgs):          4335 seconds.
SGD:                     568 seconds.
ASGD:                    150 seconds.

Note that crf++ implements an alternate algorithm (mira) that is related to the
stochastic gradient descent but optimizes a different loss function.





